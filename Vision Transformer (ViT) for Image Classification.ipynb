{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g3-o_uMwgmM"
      },
      "source": [
        "# Vision Transformer (ViT) for Image Classification [5 points]\n",
        "Use a Vision Transformer to solve the Cats and Dogs Dataset. You can use pre-defined ViT model or implement from scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM0VLUfAwgmS"
      },
      "source": [
        "## Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JN0wGFJvaySC"
      },
      "outputs": [],
      "source": [
        "!unzip -q kagglecatsanddogs_5340.zip -d /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUB8ZXycbDgk",
        "outputId": "d605e629-e0b6-4b12-d40b-e57a7be25ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data_dir = /content/PetImages\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "root='/content'\n",
        "data_dir=None\n",
        "for dirpath, dirnames,_ in os.walk(root):\n",
        "    if 'Cat' in dirnames and 'Dog' in dirnames:\n",
        "        data_dir=dirpath\n",
        "        break\n",
        "if data_dir is None:\n",
        "    raise FileNotFoundError(f\"Couldn’t locate the PetImages folder under {root}\")\n",
        "print(f\"data_dir = {data_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goi55k86wgmS"
      },
      "source": [
        "1. Load and preprocess the dataset. This may include resizing images, normalizing pixel values, and splitting the dataset into training, validation, and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjfRNnq07uFW",
        "outputId": "f7010d18-62fd-4127-b1cd-6bf775e7cb85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing bad file: /content/PetImages/Cat/Thumbs.db\n",
            "Removing bad file: /content/PetImages/Cat/666.jpg\n",
            "Removing bad file: /content/PetImages/Dog/Thumbs.db\n",
            "Removing bad file: /content/PetImages/Dog/11702.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:950: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sizes → Train: 17498, Val: 3749, Test: 3751\n",
            "shapes: torch.Size([32, 3, 224, 224]) torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image, UnidentifiedImageError\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import os\n",
        "def clean_folder(folder):\n",
        "    for fname in os.listdir(folder):\n",
        "        path=os.path.join(folder,fname)\n",
        "        try:\n",
        "            with Image.open(path) as img:\n",
        "                img.verify()\n",
        "        except (UnidentifiedImageError,OSError):\n",
        "            print(\"Removing bad file:\",path)\n",
        "            os.remove(path)\n",
        "\n",
        "for cls in ['Cat','Dog']:\n",
        "    clean_folder(os.path.join(data_dir,cls))\n",
        "train_tfms=transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "test_tfms=transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "base_ds=datasets.ImageFolder(data_dir)\n",
        "num_samples=len(base_ds)\n",
        "indices=np.arange(num_samples)\n",
        "np.random.seed(42); np.random.shuffle(indices)\n",
        "train_end=int(0.7*num_samples)\n",
        "val_end=train_end+int(0.15*num_samples)\n",
        "\n",
        "train_idx=indices[:train_end]\n",
        "val_idx=indices[train_end:val_end]\n",
        "test_idx=indices[val_end:]\n",
        "\n",
        "train_ds=Subset(datasets.ImageFolder(data_dir,transform=train_tfms),train_idx)\n",
        "val_ds=Subset(datasets.ImageFolder(data_dir,transform=test_tfms),val_idx)\n",
        "test_ds=Subset(datasets.ImageFolder(data_dir,transform=test_tfms),test_idx)\n",
        "\n",
        "batch_size=32\n",
        "train_loader=DataLoader(train_ds,batch_size=batch_size,shuffle=True,num_workers=2)\n",
        "val_loader=DataLoader(val_ds,batch_size=batch_size,shuffle=False,num_workers=2)\n",
        "test_loader=DataLoader(test_ds,batch_size=batch_size,shuffle=False,num_workers=2)\n",
        "\n",
        "print(f\"Sizes → Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
        "imgs, labels=next(iter(train_loader))\n",
        "print(\"shapes:\",imgs.shape,labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hxJBjDmwgmU"
      },
      "source": [
        "2. Choose to use a pre-defined ViT model or implement it from scratch. You can use an in-built predefined models for this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "48e0afb67e3d41ff99d8fce78ce07a07",
            "6088d81bf82d49c4ae40c5e1dd5fb538",
            "5998e4af0e424a30b071bd28237d750e",
            "534c1612bd6b4a3b9a292dd411093d76",
            "5a5ab10cb1f744f5be60406911563023",
            "a75425cca60e41e7892d7b9ffed48f6b",
            "38f03ad066d94cae9bd286fe71a379cd",
            "9209f4f0991943f8979e446b27a68107",
            "ad8cff3c6ce841b3a7e0a89792ec05c3",
            "85678a0fcec942e999a0e9f002fb06c8",
            "6ed09e2aa7bb42b8a12c7255f80c462a"
          ]
        },
        "id": "t_5j-iDjwgmV",
        "outputId": "da29aa80-ef57-4f40-f46d-75c3338f0747"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48e0afb67e3d41ff99d8fce78ce07a07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vit_base_patch32_224 with 87.5M params.\n",
            "VisionTransformer(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
            "    (norm): Identity()\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (patch_drop): Identity()\n",
            "  (norm_pre): Identity()\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (1): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (2): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (3): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (4): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (5): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (6): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (7): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (8): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (9): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (10): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (11): Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): Identity()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): Identity()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "  (fc_norm): Identity()\n",
            "  (head_drop): Dropout(p=0.0, inplace=False)\n",
            "  (head): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import timm\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name=\"vit_base_patch32_224\"\n",
        "model=timm.create_model(model_name,pretrained=True,num_classes=2)\n",
        "\n",
        "model.to(device)\n",
        "print(\"Loaded {} with {:.1f}M params.\".format(model_name, sum(p.numel() for p in model.parameters()) / 1e6))\n",
        "\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-2)\n",
        "scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOtcOeFVwgmV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCJcj6XhwgmV"
      },
      "source": [
        "3. Train and evaluate your ViT model. Discuss your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tf3Oh8VwgmW",
        "outputId": "8d1615d5-4eea-46b1-9c91-1999654e56bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch1/5Train Loss:0.0496Acc:0.9823 |Val Loss: 0.0289  Acc: 0.9880\n",
            "Epoch2/5Train Loss:0.0112Acc:0.9962 |Val Loss: 0.0447  Acc: 0.9869\n",
            "  → No improvement for 1/3 epochs\n",
            "Epoch3/5Train Loss:0.0080Acc:0.9975 |Val Loss: 0.0356  Acc: 0.9899\n",
            "Epoch4/5Train Loss:0.0047Acc:0.9982 |Val Loss: 0.0546  Acc: 0.9843\n",
            "  → No improvement for 1/3 epochs\n",
            "Epoch5/5Train Loss:0.0033Acc:0.9991 |Val Loss: 0.0288  Acc: 0.9917\n",
            "\n",
            "Training complete. Best val_acc: 0.9917 (epoch 5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:950: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Accuracy: 0.9917\n",
            "\n",
            "Classification Report of the Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Cat       0.99      0.99      0.99      1859\n",
            "         Dog       0.99      0.99      0.99      1892\n",
            "\n",
            "    accuracy                           0.99      3751\n",
            "   macro avg       0.99      0.99      0.99      3751\n",
            "weighted avg       0.99      0.99      0.99      3751\n",
            "\n",
            "Confusion Matrix of the Model:\n",
            "[[1838   21]\n",
            " [  10 1882]]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs=5\n",
        "patience=3\n",
        "epochs_no_improve=0\n",
        "best_val_acc=0.0\n",
        "best_epoch=0\n",
        "history={'train_loss':[],'train_acc':[],'val_loss':[],'val_acc':[]}\n",
        "\n",
        "for epoch in range(1,num_epochs+1):\n",
        "    model.train()\n",
        "    running_loss=0.0\n",
        "    running_corrects=0\n",
        "\n",
        "    for images,labels in train_loader:\n",
        "        images=images.to(device)\n",
        "        labels=labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=model(images)\n",
        "        loss=criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        preds=outputs.argmax(dim=1)\n",
        "        running_loss+=loss.item()*images.size(0)\n",
        "        running_corrects+=torch.sum(preds==labels.data)\n",
        "\n",
        "    train_loss=running_loss/len(train_loader.dataset)\n",
        "    train_acc=running_corrects.double()/len(train_loader.dataset)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc.item())\n",
        "\n",
        "    model.eval()\n",
        "    val_loss=0.0\n",
        "    val_corrects=0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images,labels in val_loader:\n",
        "            images=images.to(device)\n",
        "            labels=labels.to(device)\n",
        "\n",
        "            outputs=model(images)\n",
        "            loss=criterion(outputs, labels)\n",
        "\n",
        "            preds=outputs.argmax(dim=1)\n",
        "            val_loss+=loss.item()*images.size(0)\n",
        "            val_corrects+=torch.sum(preds==labels.data)\n",
        "\n",
        "    val_loss=val_loss/len(val_loader.dataset)\n",
        "    val_acc=val_corrects.double() / len(val_loader.dataset)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc.item())\n",
        "    scheduler.step()\n",
        "    print(\"Epoch{}/{}Train Loss:{:.4f}Acc:{:.4f} |Val Loss: {:.4f}  Acc: {:.4f}\".format(epoch,num_epochs,train_loss,train_acc,val_loss,val_acc))\n",
        "    if val_acc>best_val_acc:\n",
        "        best_val_acc=val_acc\n",
        "        best_epoch=epoch\n",
        "        epochs_no_improve=0\n",
        "        torch.save(model.state_dict(),\"best_vit_cats_vs_dogs.pth\")\n",
        "    else:\n",
        "        epochs_no_improve+=1\n",
        "        print(f\"  → No improvement for {epochs_no_improve}/{patience} epochs\")\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"\\nEarly stopping triggered. Stopping at epoch {epoch}.\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nTraining complete. Best val_acc: {best_val_acc:.4f} (epoch {best_epoch})\")\n",
        "model.load_state_dict(torch.load(\"best_vit_cats_vs_dogs.pth\"))\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for images,labels in test_loader:\n",
        "        images=images.to(device)\n",
        "        outputs=model(images)\n",
        "        preds=outputs.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "test_acc=np.mean(np.array(all_preds)==np.array(all_labels))\n",
        "print(f\"\\nTesting Accuracy: {test_acc:.4f}\\n\")\n",
        "\n",
        "print(\"Classification Report of the Model:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=base_ds.classes))\n",
        "\n",
        "print(\"Confusion Matrix of the Model:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwLnmGDlwgmW"
      },
      "source": [
        "<span style='color:green'>The model demonstrates strong learning and generalization on the Cat vs. Dog task. The training accuracy increased steadily from 98.2% in epoch 1 to 99.9% by epoch 5, while training loss fell from 0.0496 to 0.003. Validation accuracy reached 99.17% in epoch 5, closely following the training trend. This indicates minimal overfitting, even though there were small rises in validation loss at epochs 2 and 4. On the held-out test set, it achieves 99.17% accuracy, with balanced precision, recall, and F1-scores (all are nearly 0.99) for both classes. The confusion matrix supports this high performance, showing only 21 Cats misclassified as Dogs and 10 Dogs misclassified as Cats out of 3,751 samples. Overall, the model is very accurate and reliable.</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzodOFVWwgmX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTRgaVJzwgmZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxlsrMQxwgmZ"
      },
      "source": [
        "6. References. Include details on all the resources used to complete this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9COLF1pwgmZ"
      },
      "source": [
        "<span style='color:green'>Kaggle. Dogs vs. Cats dataset. 2013. https://www.microsoft.com/en-us/download/details.aspx?id=54765\n",
        "ViT transformer https://huggingface.co/docs/transformers/model_doc/vit\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKKGXC0WwgmZ"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "38f03ad066d94cae9bd286fe71a379cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48e0afb67e3d41ff99d8fce78ce07a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6088d81bf82d49c4ae40c5e1dd5fb538",
              "IPY_MODEL_5998e4af0e424a30b071bd28237d750e",
              "IPY_MODEL_534c1612bd6b4a3b9a292dd411093d76"
            ],
            "layout": "IPY_MODEL_5a5ab10cb1f744f5be60406911563023"
          }
        },
        "534c1612bd6b4a3b9a292dd411093d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85678a0fcec942e999a0e9f002fb06c8",
            "placeholder": "​",
            "style": "IPY_MODEL_6ed09e2aa7bb42b8a12c7255f80c462a",
            "value": " 353M/353M [00:00&lt;00:00, 393MB/s]"
          }
        },
        "5998e4af0e424a30b071bd28237d750e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9209f4f0991943f8979e446b27a68107",
            "max": 352911016,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad8cff3c6ce841b3a7e0a89792ec05c3",
            "value": 352911016
          }
        },
        "5a5ab10cb1f744f5be60406911563023": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6088d81bf82d49c4ae40c5e1dd5fb538": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a75425cca60e41e7892d7b9ffed48f6b",
            "placeholder": "​",
            "style": "IPY_MODEL_38f03ad066d94cae9bd286fe71a379cd",
            "value": "model.safetensors: 100%"
          }
        },
        "6ed09e2aa7bb42b8a12c7255f80c462a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85678a0fcec942e999a0e9f002fb06c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9209f4f0991943f8979e446b27a68107": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a75425cca60e41e7892d7b9ffed48f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad8cff3c6ce841b3a7e0a89792ec05c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
